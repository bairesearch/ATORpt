# -*- coding: utf-8 -*-
"""vit_torch-multKeys.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nR579YNHDniJVxpU77_JKTu_uTKTib68
"""

#@title globalDefs { form-width: "5%" }

import numpy as np

from tqdm import tqdm

import torch
import torch.nn as nn
from torch.nn import CrossEntropyLoss
from torch.optim import Adam
from torch.utils.data import DataLoader

from torchvision.datasets.mnist import MNIST
from torchvision.transforms import ToTensor
import torchvision.transforms as T

import torch.nn.functional as F

torch.set_printoptions(profile="full")

torch.autograd.set_detect_anomaly(True)


useGeometricHashing = True #current #mandatory
if(useGeometricHashing):
    useGeometricHashingProbabilisticKeypoints = False   #for backprop   #else use topk  #optional
    useGeometricHashingCNNfeatureDetector = False   #else use key k #optional
    useGeometricHashingPixels = False    #else perform hashing via patches  #optional
    if(useGeometricHashingCNNfeatureDetector):
        useGeometricHashingPixels = True    #currently mandatory (because W/H dim are required by CNN)
    if(useGeometricHashingProbabilisticKeypoints):
        useGeometricHashingAMANN = True #mandatory  #else use hardcoded geohashing function
        useGeometricHashingProbabilisticKeypointsSoftMax = False
        useGeometricHashingProbabilisticKeypointsNonlinearity = True
        if(useGeometricHashingProbabilisticKeypointsNonlinearity):
            if(useGeometricHashingProbabilisticKeypointsSoftMax):
                useGeometricHashingProbabilisticKeypointsNonlinearityOffset = 0.022    #input Z is normalised between 0 and 1 #calibrate
            else:
                useGeometricHashingProbabilisticKeypointsNonlinearityOffset = 1.0  #calibrate
        useGeometricHashingProbabilisticKeypointsZero = True    #zero all keypoints below attention threshold  #CHECKTHIS: affects backprop 
        if(useGeometricHashingAMANN):
            useGeometricHashingNormaliseOutput = True    #normalise geometric hashed positional embeddings output from 0 to 1
            useGeometricHashingReduceInputMagnitude = False
    else:
        useGeometricHashingAMANN = True #mandatory  #else use hardcoded geohashing function
        if(useGeometricHashingAMANN):
            useGeometricHashingNormaliseOutput = True
            useGeometricHashingReduceInputMagnitude = False #reduce average magnitude of positional embedding input
    useGeometricHashingKeypointNormalisation = True
    numberOfGeometricDimensions = 2    #2D object data (2DOD)

#useMultKeys = False   #legacy

activationMaxVal = 10.0
multiplicativeEmulationFunctionOffsetVal = 1.0	#add/subtract
multiplicativeEmulationFunctionPreMinVal = 1e-9
multiplicativeEmulationFunctionPreMaxVal = 1e+9	#or activationMaxVal (effective)
multiplicativeEmulationFunctionPostMaxVal = 20.0

#@title operations { form-width: "5%" }

def getInputLayerNumTokens(numberOfPatches):
    inputLayerNumTokens = int(numberOfPatches ** 2)
    return inputLayerNumTokens

def getInputDim(inputShape, patchSize):
    c = inputShape[0]
    numberOfInputDimensions = int(c * getPatchSizeFlat2(patchSize))
    return numberOfInputDimensions

def getPatchSize(inputShape, numberOfPatches):
    patchSize = (inputShape[1]//numberOfPatches, inputShape[2]//numberOfPatches)
    return patchSize

def getPatchLength(inputShape, numberOfPatches):
    (c, h, w) = inputShape
    patchLength = h // numberOfPatches
    return patchLength

def getPatchSizeFlat(inputShape, numberOfPatches):
    patchSize = getPatchSize(inputShape, numberOfPatches)
    patchSizeFlat = getPatchSizeFlat2(patchSize)
    return patchSizeFlat

def getPatchSizeFlat2(patchSize):
    patchSizeFlat = patchSize[0]*patchSize[1]
    return patchSizeFlat

def normaliseInputs0to1(A):
    if(useGeometricHashingKeypointNormalisation):
        A = F.normalize(A)    
    return A

def getPositionalEmbeddingsAbsolute(numberOfPatches):
    #see createLinearPatches specification; patches[imageIndex, i*numberOfPatches + j]
    posEmbeddingsAbsolute = torch.zeros(getInputLayerNumTokens(numberOfPatches), 2)  #pos embeddings absolute include x/y dim only
    posEmbeddingsAbsolute[:, 0] = torch.unsqueeze(torch.arange(1, numberOfPatches+1),1).repeat(1, numberOfPatches).flatten()
    posEmbeddingsAbsolute[:, 1] = torch.arange(1, numberOfPatches+1).repeat(numberOfPatches)

    return posEmbeddingsAbsolute

def createLinearPatches(images, numberOfPatches, flattenChannels=True):
    n, c, h, w = images.shape
    inputShape = (c, h, w)
 
    if(h != w):
        print("createLinearPatches requires h == w")

    if(flattenChannels):
        patches = torch.zeros(n, getInputLayerNumTokens(numberOfPatches), getPatchSizeFlat(inputShape, numberOfPatches))
    else:
        patches = torch.zeros(n, c, getInputLayerNumTokens(numberOfPatches), getPatchSizeFlat(inputShape, numberOfPatches))

    patchLength = getPatchLength(inputShape, numberOfPatches)

    for imageIndex, image in enumerate(images):
        for i in range(numberOfPatches):
            for j in range(numberOfPatches):
                patch = image[:, i*patchLength:(i+1)*patchLength, j*patchLength:(j+1)*patchLength]
                if(flattenChannels):
                    patch = patch.flatten()
                    patches[imageIndex, i*numberOfPatches + j] = patch
                else:
                    patch = patch.flatten(start_dim=1, end_dim=2)
                    for k in range(c):
                        patches[imageIndex, k, i*numberOfPatches + j] = patch[k]
    return patches

def uncreateLinearPatches(sequences, numberOfPatches, numberOfGeoDimensions):
    n = sequences.shape[0]
    seqLength = sequences.shape[1]  #numberOfPatches*numberOfPatches

    images = torch.zeros(n, numberOfPatches, numberOfPatches, numberOfGeoDimensions)

    for imageIndex, sequence in enumerate(sequences):
        for i in range(numberOfPatches):
            for j in range(numberOfPatches):
                for k in range(numberOfGeometricDimensions):
                    images[imageIndex, i, j, k] = sequence[i*numberOfPatches + j, k]

    return images

#@title LayerAdditiveMultiplicativeClass (AMANN)  { form-width: "5%" }

class LayerAdditiveMultiplicativeClass(nn.Module):
    def __init__(self, inputFeatures, outputFeatures, useBias=False, useMultiplicativeUnits=True):
        super().__init__()
        self.inputFeatures = inputFeatures
        self.outputFeatures = outputFeatures
        self.useBias = useBias
        self.useMultiplicativeUnits = useMultiplicativeUnits

        if(useMultiplicativeUnits):
            self.inputFeaturesAdditive = inputFeatures
            self.outputFeaturesAdditive = outputFeatures//2
            self.inputFeaturesMultiplicative = inputFeatures
            self.outputFeaturesMultiplicative = outputFeatures//2

            self.Wa = torch.nn.Parameter(torch.randn(self.inputFeaturesAdditive, self.outputFeaturesAdditive))
            self.Wm = torch.nn.Parameter(torch.randn(self.inputFeaturesMultiplicative, self.outputFeaturesMultiplicative))
            if(self.useBias):
                self.Ba = torch.nn.Parameter(torch.zeros(self.outputFeaturesAdditive))  #randn
                self.Bm = torch.nn.Parameter(torch.zeros(self.outputFeaturesMultiplicative))    #randn
        else:
            self.W = torch.nn.Parameter(torch.randn(self.inputFeatures, self.outputFeatures))
            if(self.useBias):
                self.B = torch.nn.Parameter(torch.zeros(self.outputFeatures))   #randn
 
        self.activationFunction = torch.nn.ReLU()

    def forward(self, input):
        x = input
        if(self.useMultiplicativeUnits):
            AprevLayerA = x
            AprevLayerA = self.clipActivation(AprevLayerA)
            AprevLayerM = self.multiplicativeEmulationFunctionPre(AprevLayerA)
            #print("self.Wa.shape = ", self.Wa.shape)
            #print("self.Wm.shape = ", self.Wm.shape)
            Za = AprevLayerA @ self.Wa
            Zm = AprevLayerM @ self.Wm
            Zm = self.multiplicativeEmulationFunctionPost(Zm)
            if(self.useBias):
                Za = Za + self.Ba
                Zm = Zm + self.Bm
 
            Aa = self.activationFunction(Za)
            Am = self.activationFunction(Zm)
            Z = torch.cat([Za, Zm], dim=1)
            A = torch.cat([Aa, Am], dim=1)
            output = A
            if(torch.isnan(A).any()):
                print("torch.isnan(A).any()")
                ex
        else:
            A = x @ self.W
            if(self.useBias):
                A = A + self.B
            output = self.activationFunction(A)
        return output

    def clipActivation(self, A):
        A = torch.clip(A, -activationMaxVal, activationMaxVal)	
        return A

    def multiplicativeEmulationFunctionPre(self, AprevLayer):
        AprevLayer = AprevLayer + multiplicativeEmulationFunctionOffsetVal
        AprevLayer = torch.clip(AprevLayer, multiplicativeEmulationFunctionPreMinVal, multiplicativeEmulationFunctionPreMaxVal)	
        AprevLayerM = torch.log(AprevLayer)
        return AprevLayerM
        
    def multiplicativeEmulationFunctionPost(self, ZmIntermediary):
        ZmIntermediary = torch.clip(ZmIntermediary, -multiplicativeEmulationFunctionPostMaxVal, multiplicativeEmulationFunctionPostMaxVal)
        Zm = torch.exp(ZmIntermediary)
        Zm = Zm - multiplicativeEmulationFunctionOffsetVal
        return Zm

#@title FeatureDetectorCNNClass  { form-width: "5%" }

class FeatureDetectorCNNClass(nn.Module):
    def __init__(self, numberOfPatches):
        super(FeatureDetectorCNNClass, self).__init__()

        self.numberOfPatches = numberOfPatches
        self.numChannels = 16
        self.featureDetector = nn.Sequential(
                nn.Conv2d(in_channels=1, out_channels=self.numChannels, kernel_size=5, stride=1, padding='same'),
                nn.ReLU(),
                nn.Conv2d(in_channels=self.numChannels, out_channels=1, kernel_size=3, stride=1, padding='same'),
                nn.ReLU()
        )

    def forward(self, images):
        featureMap = self.featureDetector(images)

        #convert 2D to linear;
        featureMap = featureDetectorInput = createLinearPatches(featureMap, self.numberOfPatches)

        return featureMap

#@title FeatureDetectorMSAClass  { form-width: "5%" }

class FeatureDetectorMSAClass(nn.Module):
    def __init__(self, numberOfTokenDimensions, numberOfPatches):
        super(FeatureDetectorMSAClass, self).__init__()

        self.numberOfPatches = numberOfPatches

        self.numberOfTokenDimensions = numberOfTokenDimensions

        #self.qWeights = nn.Linear(numberOfTokenDimensions, numberOfTokenDimensions)
        self.kWeights = nn.Linear(numberOfTokenDimensions, numberOfTokenDimensions)
        #self.vWeights = nn.Linear(numberOfTokenDimensions, numberOfTokenDimensions)

        self.featureDetector = nn.Linear(numberOfTokenDimensions, 1)

    def forward(self, sequences):
        #sequences shape (N, sequenceLength, numberOfTokenDimensions)
  
        featureMapList = []
        batchSize = sequences.shape[0]
        sequenceLength = sequences.shape[1]

        posEmbeddingsAbsolute = getPositionalEmbeddingsAbsolute(self.numberOfPatches).repeat(batchSize, 1, 1)

        for N in range(batchSize):

            seq = sequences[N]
            posEmbeddings = posEmbeddingsAbsolute[N]
            
            #q = self.qWeights(seq)
            k = self.kWeights(seq)
            #v = self.vWeights(seq)
            
            featureMapN = self.featureDetector(k) #49x16 -> 49x1

            #featureMap = q @ k.T   #49x16 -> 49x49
            #kdots = k @ k.T
            #dots = q @ kdots.T
            
            featureMapList.append(featureMapN)

        featureMap = torch.stack(featureMapList, dim=0) 

        return featureMap

#@title GeometricHashingClass  { form-width: "5%" }

class GeometricHashingClass(nn.Module):
    def __init__(self, numberOfTokenDimensions, numberOfPatches):

        self.numberOfPatches = numberOfPatches
        self.numberOfTokenDimensions = numberOfTokenDimensions

        super(GeometricHashingClass, self).__init__()
          
        self.cosSim = torch.nn.CosineSimilarity(dim=1)  #CHECKTHIS: dim=1

        self.numberOfGeometricDimensions = numberOfGeometricDimensions    #2D object data (2DOD)
        self.geometricHashingNumKeypoints = self.numberOfGeometricDimensions+1    #2DOD: 3: 3DOD: 4   #number of features to use to perform geometric hashing (depends on input object data dimensions; 2DOD/3DOD)
        self.geometricHashingNumPixels = 1  #1 pixel (token) will be transformed
        
        if(useGeometricHashingAMANN):
            self.geometricHashingNumberLayers = self.geometricHashingNumKeypoints #number of consecutive transformations required to be encoded/learnt by MSAgeomtricHashing
            if(useGeometricHashingProbabilisticKeypoints):
                if(useGeometricHashingProbabilisticKeypointsSoftMax):
                    self.softmax = nn.Softmax(dim=-1)
                if(useGeometricHashingProbabilisticKeypointsNonlinearity):
                    self.activationFunction = torch.nn.ReLU(inplace=False)
                
                inputLayerNumTokens = getInputLayerNumTokens(numberOfPatches)
                self.numberOfAttentionDimensions = 1
                self.geometricHashingNumInputs = inputLayerNumTokens + 1
                self.geometricHashingInputDim = (self.geometricHashingNumInputs*(self.numberOfGeometricDimensions+self.numberOfAttentionDimensions))
            else:  
                self.geometricHashingNumInputs = self.geometricHashingNumKeypoints+self.geometricHashingNumPixels
                self.geometricHashingInputDim = self.geometricHashingNumInputs * self.numberOfGeometricDimensions
     
            linearAdditiveMultiplicativeList = []
            for i in range(self.geometricHashingNumberLayers):
                linearAdditiveMultiplicativeList.append(LayerAdditiveMultiplicativeClass(self.geometricHashingInputDim, self.geometricHashingInputDim, useMultiplicativeUnits=True))
            linearAdditiveMultiplicativeList.append(LayerAdditiveMultiplicativeClass(self.geometricHashingInputDim, self.numberOfGeometricDimensions, useMultiplicativeUnits=False))
            self.linearAdditiveMultiplicativeModuleList = nn.ModuleList(linearAdditiveMultiplicativeList)


    def modifiedReLU(self, Z):
        Z = Z - useGeometricHashingProbabilisticKeypointsNonlinearityOffset
        A = self.activationFunction(Z)
        return A

    def forward(self, sequences, featureMap):

        posEmbeddingsGeometricNormalisedList = []
        batchSize = sequences.shape[0]
        sequenceLength = sequences.shape[1]

        posEmbeddings = getPositionalEmbeddingsAbsolute(self.numberOfPatches)

        #sequences shape (N, sequenceLength, numberOfTokenDimensions)

        for N in range(batchSize):

            #proximity calculations;

            #https://stackoverflow.com/questions/29063851/how-to-parallelized-scipy-cosine-similarity-calculation
            posEmbeddings1 = posEmbeddings/torch.linalg.norm(posEmbeddings, axis=1)[:,None]
            posEmbeddings1 = torch.nan_to_num(posEmbeddings1, nan=0.0)  #CHECKTHIS; nan=0.0

            dotsProximity = torch.einsum('ik,jk->ij', posEmbeddings1, posEmbeddings1)

            dotsLuminosity = featureMap[N]
            #print("dotsLuminosity = ", dotsLuminosity.shape)
            #print("dotsProximity = ", dotsProximity.shape)
            dots = torch.multiply(dotsLuminosity, dotsProximity)

            posEmbeddingsNormalised = normaliseInputs0to1(posEmbeddings)
            geometricHashingPixelPosEmbeddings = posEmbeddingsNormalised

            if(useGeometricHashingProbabilisticKeypoints):
                attention = dots
                if(useGeometricHashingProbabilisticKeypointsSoftMax):
                    attention = self.softmax(attention / (self.numberOfTokenDimensions ** 0.5))   #attention = self.softmax(dots)
                if(useGeometricHashingProbabilisticKeypointsNonlinearity):
                    attention = self.modifiedReLU(attention) #apply non-linearity to select small number of keypoints    #self.activationFunction(dots)

                attention = torch.unsqueeze(attention, dim=2)
                
                posEmbeddingsRepeat = posEmbeddingsNormalised
                posEmbeddingsRepeat = torch.unsqueeze(posEmbeddingsRepeat, dim=1)
                numRep = posEmbeddingsRepeat.shape[0]
                posEmbeddingsRepeat = posEmbeddingsRepeat.repeat(1, posEmbeddingsRepeat.shape[0], 1)   

                if(useGeometricHashingProbabilisticKeypointsZero):
                    attentionThresholded = torch.gt(attention, 0.0).type(torch.float) 
                    posEmbeddingsRepeat = torch.multiply(posEmbeddingsRepeat, attentionThresholded)

                geometricHashingKeypointsPosEmbeddings = torch.cat([posEmbeddingsRepeat, attention], dim=2)

                pixelPosEmbeddingAttentionArtificial = torch.ones(1,1).repeat(sequenceLength, 1)
                geometricHashingPixelPosEmbeddings = torch.cat([geometricHashingPixelPosEmbeddings, pixelPosEmbeddingAttentionArtificial], dim=1)    #set pixel embedding artificial attention value to 1.0
            else:
                keypoints = torch.topk(dots, k=self.geometricHashingNumKeypoints, dim=1)
                keypointsIndices = keypoints.indices
                keypointsValues = keypoints.values
                
                keypointsIndicesFlattened = torch.reshape(keypointsIndices, (keypointsIndices.shape[0]*keypointsIndices.shape[1],))  #or flatten    #keypointsIndicesFlattened = keypointsIndices.flatten()
                keypointsPosEmbeddingsFlattened = posEmbeddingsNormalised[keypointsIndicesFlattened]
                keypointsPosEmbeddings = torch.reshape(keypointsPosEmbeddingsFlattened, (keypointsIndices.shape[0], keypointsIndices.shape[1], self.numberOfGeometricDimensions))  #CHECKTHIS
                geometricHashingKeypointsPosEmbeddings = keypointsPosEmbeddings
       
            
            if(useGeometricHashingAMANN):
                geometricHashingKeypointsPosEmbeddings = geometricHashingKeypointsPosEmbeddings.flatten(start_dim=1, end_dim=2)

                geometricHashingInputs = torch.cat([geometricHashingKeypointsPosEmbeddings, geometricHashingPixelPosEmbeddings], dim=1)

                if(useGeometricHashingReduceInputMagnitude):
                    geometricHashingInputs = geometricHashingInputs / 5.0

                geometricHashingLayer = geometricHashingInputs
                for i, l in enumerate(self.linearAdditiveMultiplicativeModuleList):
                    geometricHashingLayer = l(geometricHashingLayer)
                    #print("geometricHashingLayer = ", geometricHashingLayer)
                geometricHashingOutput = geometricHashingLayer
                #print("geometricHashingOutput = ", geometricHashingOutput)

                posEmbeddingsAbsoluteGeoNormalisedN = geometricHashingOutput

                if(useGeometricHashingNormaliseOutput):
                    posEmbeddingsAbsoluteGeoNormalisedN = normaliseInputs0to1(posEmbeddingsAbsoluteGeoNormalisedN)
                #print("posEmbeddingsAbsoluteGeoNormalisedN = ", posEmbeddingsAbsoluteGeoNormalisedN)


            posEmbeddingsGeometricNormalisedList.append(posEmbeddingsAbsoluteGeoNormalisedN)

        posEmbeddingsGeometricNormalised = torch.stack(posEmbeddingsGeometricNormalisedList, dim=0) 

        return posEmbeddingsGeometricNormalised

#@title ViTgeometricHashingClass { form-width: "5%" }

class ViTgeometricHashingClass(nn.Module):
    def __init__(self, inputShape, numberOfPatches, numberOfHiddenDimensions, numberOfOutputDimensions):
        super(ViTgeometricHashingClass, self).__init__()

        self.inputShape = inputShape
        c, w, h = inputShape
        self.numberOfPatches = numberOfPatches

        self.patchSize = getPatchSize(inputShape, numberOfPatches)

        self.numberOfInputDimensions = getInputDim(inputShape, self.patchSize)
        self.numberOfHiddenDimensions = numberOfHiddenDimensions
        self.numberOfOutputDimensions = numberOfOutputDimensions

        if(useGeometricHashingPixels):
            self.featureDetectorPatchSize = getPatchSize(inputShape, w)
            self.featureDetectorInputDim = getInputDim(inputShape, self.featureDetectorPatchSize)
            self.featureDetectorNumPatches = w
        else:
            self.featureDetectorInputDim = self.numberOfInputDimensions
            self.featureDetectorNumPatches = numberOfPatches

        self.sequenceLength = getInputLayerNumTokens(numberOfPatches)
        
        if(useGeometricHashingCNNfeatureDetector):
            self.featureDetectorCNN = FeatureDetectorCNNClass(self.featureDetectorNumPatches)
        else:
            self.featureDetectorMSA = FeatureDetectorMSAClass(self.featureDetectorInputDim, self.featureDetectorNumPatches)

        self.geometricHashing = GeometricHashingClass(self.featureDetectorInputDim, self.featureDetectorNumPatches)

        self.outputLayer = nn.Sequential(
            nn.Linear(self.numberOfHiddenDimensions*self.sequenceLength, self.numberOfOutputDimensions), 
            nn.Softmax(dim=-1))

    def forward(self, images):

        n, c, w, h = images.shape
        tokens = createLinearPatches(images, self.numberOfPatches)

        if(useGeometricHashingPixels):
            if(useGeometricHashingCNNfeatureDetector):
                featureDetectorInput = images
            else:
                featureDetectorInput = createLinearPatches(images, w)
        else:
            featureDetectorInput = tokens

        if(useGeometricHashingCNNfeatureDetector):
            featureMap = self.featureDetectorCNN(featureDetectorInput)
        else:
            featureMap = self.featureDetectorMSA(featureDetectorInput)

        posEmbeddingsAbsoluteGeoNormalised = self.geometricHashing(featureDetectorInput, featureMap)

        if(useGeometricHashingPixels):
            #convert pixel positional embeddings to patch positional embeddings
            imagePosEmbeddings = uncreateLinearPatches(posEmbeddingsAbsoluteGeoNormalised, self.featureDetectorNumPatches, numberOfGeometricDimensions)
            imagePosEmbeddings = imagePosEmbeddings.permute(0, 3, 1, 2) #change to n, numberOfGeoDimensions, x, y
            posEmbeddingsAbsoluteGeoNormalised = createLinearPatches(imagePosEmbeddings, self.numberOfPatches, flattenChannels=False)
            posEmbeddingsAbsoluteGeoNormalised = posEmbeddingsAbsoluteGeoNormalised.permute(0, 2, 3, 1) #change to n, seqLength, numberOfInputDimensions, numberOfGeoDimensions
            posEmbeddingsAbsoluteGeoNormalised = torch.mean(posEmbeddingsAbsoluteGeoNormalised, dim=2)  #average over numberOfInputDimensions
        
        tokensAndPosEmbeddings = torch.cat([tokens, posEmbeddingsAbsoluteGeoNormalised], dim=2)

        finalHiddenLayer = torch.reshape(tokensAndPosEmbeddings, (tokensAndPosEmbeddings.shape[0], tokensAndPosEmbeddings.shape[1]*tokensAndPosEmbeddings.shape[2]))
        pred = self.outputLayer(finalHiddenLayer)

        return pred

#@title main  { form-width: "5%" }

def main():

    #template: https://github.com/BrianPulfer/PapersReimplementations/blob/master/vit/vit_torch.py

    trainDataset = MNIST(root='./../datasets', train=True, download=True, transform=ToTensor())
    testDataset = MNIST(root='./../datasets', train=False, download=True, transform=ToTensor())

    batchSize = 16 #debug: 2
    inputShape = (1, 28, 28)   #MNIST defined
    numberOfOutputDimensions = 10     #MNIST defined

    trainDataLoader = DataLoader(trainDataset, shuffle=True, batch_size=batchSize)
    testDataLoader = DataLoader(testDataset, shuffle=False, batch_size=batchSize)

    numberOfPatches = 7
    patchSize = getPatchSize(inputShape, numberOfPatches)
    numberOfInputDimensions = getInputDim(inputShape, patchSize)
    numberOfHiddenDimensions = numberOfInputDimensions + numberOfGeometricDimensions   #mandatory (currently required for getPositionalEmbeddingsAbsolute, as no method implemented for mapping between hashing_d and numberOfHiddenDimensions)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    geometricHashingModel = ViTgeometricHashingClass(inputShape, numberOfPatches, numberOfHiddenDimensions, numberOfOutputDimensions).to(device)

    numberOfEpochs = 10
    learningRate = 0.05

    optimizer = Adam(geometricHashingModel.parameters(), lr=learningRate)
    criterion = CrossEntropyLoss()
    for epoch in tqdm(range(numberOfEpochs), desc="Train"):
        trainLoss = 0.0
        for batch in tqdm(trainDataLoader, desc=f"Epoch {epoch + 1}", leave=False):
            x, y = batch
            x, y = x.to(device), y.to(device)

            posEmbeddingsAbsoluteGeoNormalised = None
            if(useGeometricHashing):
                pred = geometricHashingModel(x)
            
            loss = criterion(pred, y) / len(x)

            trainLoss += loss.detach().cpu().item()

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        print(f"Epoch {epoch + 1}/{numberOfEpochs} loss: {trainLoss:.2f}")

    correct, total = 0, 0
    testLoss = 0.0
    for batch in tqdm(testDataLoader, desc="Test"):
        x, y = batch
        x, y = x.to(device), y.to(device)
        pred = geometricHashingModel(x)
        loss = criterion(y_hat, y) / len(x)
        testLoss += loss.detach().cpu().item()

        correct += torch.sum(torch.argmax(pred, dim=1) == y).detach().cpu().item()
        total += len(x)

    print(f"Test loss: {testLoss:.2f}")
    print(f"Test accuracy: {correct / total * 100:.2f}%")


if __name__ == '__main__':
    main()